{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSfPrkXnHS6eehP1Tw2PGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashanksn111/AI-ML-Developer-Assignment-/blob/main/Task01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBBa09R2ZF6-",
        "outputId": "a1af6bdd-4cba-477b-aad0-f1c5c6ed79a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting hnswlib\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.15.14-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.41-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Collecting sagemaker<3.0.0,>=2.232.1 (from cohere)\n",
            "  Downloading sagemaker-2.232.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hnswlib) (1.26.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dataclasses-json (from unstructured)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.26.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting botocore<1.36.0,>=1.35.41 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.41-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.3)\n",
            "Collecting attrs<24,>=23.1.0 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (2.2.1)\n",
            "Collecting docker (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (0.2.0)\n",
            "Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (4.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (2.2.2)\n",
            "Collecting pathos (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (4.3.6)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.12 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (3.20.3)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (6.0.2)\n",
            "Collecting sagemaker-core<2.0.0,>=1.0.0 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading sagemaker_core-1.0.10-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting sagemaker-mlflow (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading sagemaker_mlflow-0.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting schema (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: tblib<4,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.24.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.9.11)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.1)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.2.0)\n",
            "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-5.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker<3.0.0,>=2.232.1->cohere) (3.20.2)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (13.9.2)\n",
            "Collecting mock<5.0,>4.0 (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere) (0.20.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker<3.0.0,>=2.232.1->cohere) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker<3.0.0,>=2.232.1->cohere) (2024.2)\n",
            "Collecting ppft>=1.7.6.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.3.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.5 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.17 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting mlflow>=2.8 (from sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading mlflow-2.17.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Collecting mlflow-skinny==2.17.0 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading mlflow_skinny-2.17.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.2.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting graphene<4 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.7.1)\n",
            "Requirement already satisfied: pyarrow<18,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (16.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.5.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.0.35)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.1.4)\n",
            "Collecting gunicorn<24 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (5.5.0)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading databricks_sdk-0.34.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.16.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.5.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (2.18.0)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.0.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.2.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.1.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.27.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.2.14)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (71.0.4)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.37b0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.6.1)\n",
            "Downloading cohere-5.11.0-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading unstructured-0.15.14-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.41-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading sagemaker-2.232.2-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.26.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.41-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.0.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sagemaker_core-1.0.10-py3-none-any.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.4/388.4 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\n",
            "Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-2.17.0-py3-none-any.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.17.0-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.34.0-py3-none-any.whl (565 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.6/565.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: hnswlib, langdetect\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2360795 sha256=613ba6e5b8f6c60337d36bfb2904f45e25e1ad5056dc32a28f365573bf5a1e02\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=0e570a55b01001a60104a8109f21eeb435b750693111dfd63f02fd280a5f2cfc\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built hnswlib langdetect\n",
            "Installing collected packages: schema, filetype, aniso8601, types-requests, smmap, smdebug-rulesconfig, rapidfuzz, python-magic, python-iso639, pypdf, ppft, pox, parameterized, olefile, mypy-extensions, mock, marshmallow, Mako, langdetect, jsonpath-python, jmespath, importlib-metadata, httpx-sse, hnswlib, h11, gunicorn, graphql-core, fastavro, emoji, dill, backoff, attrs, typing-inspect, requests-toolbelt, python-oxmsg, multiprocess, httpcore, graphql-relay, gitdb, docker, botocore, alembic, s3transfer, pathos, httpx, graphene, gitpython, dataclasses-json, databricks-sdk, unstructured-client, mlflow-skinny, boto3, unstructured, sagemaker-core, mlflow, sagemaker-mlflow, sagemaker, cohere\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.2.0\n",
            "    Uninstalling attrs-24.2.0:\n",
            "      Successfully uninstalled attrs-24.2.0\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.3 aniso8601-9.0.1 attrs-23.2.0 backoff-2.2.1 boto3-1.35.41 botocore-1.35.41 cohere-5.11.0 databricks-sdk-0.34.0 dataclasses-json-0.6.7 dill-0.3.9 docker-7.1.0 emoji-2.14.0 fastavro-1.9.7 filetype-1.2.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.5 graphql-relay-3.2.0 gunicorn-23.0.0 h11-0.14.0 hnswlib-0.8.0 httpcore-1.0.6 httpx-0.27.2 httpx-sse-0.4.0 importlib-metadata-6.11.0 jmespath-1.0.1 jsonpath-python-1.0.6 langdetect-1.0.9 marshmallow-3.22.0 mlflow-2.17.0 mlflow-skinny-2.17.0 mock-4.0.3 multiprocess-0.70.17 mypy-extensions-1.0.0 olefile-0.47 parameterized-0.9.0 pathos-0.3.3 pox-0.3.5 ppft-1.7.6.9 pypdf-5.0.1 python-iso639-2024.4.27 python-magic-0.4.27 python-oxmsg-0.0.1 rapidfuzz-3.10.0 requests-toolbelt-1.0.0 s3transfer-0.10.3 sagemaker-2.232.2 sagemaker-core-1.0.10 sagemaker-mlflow-0.1.0 schema-0.7.7 smdebug-rulesconfig-1.0.1 smmap-5.0.1 types-requests-2.32.0.20241016 typing-inspect-0.9.0 unstructured-0.15.14 unstructured-client-0.26.1\n"
          ]
        }
      ],
      "source": [
        "pip install cohere hnswlib unstructured"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "import uuid\n",
        "import hnswlib\n",
        "from typing import List, Dict\n",
        "from unstructured.partition.html import partition_html\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "\n",
        "co = cohere.Client(\"8mcORf0RaGpY10ha2goKWluRaLSOhs15NwgY6iOc\")"
      ],
      "metadata": {
        "id": "GoJQA9K7ZrR-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_documents = [\n",
        "    {\n",
        "        \"title\": \"Crafting Effective Prompts\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"},\n",
        "    {\n",
        "        \"title\": \"Advanced Prompt Engineering Techniques\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"},\n",
        "    {\n",
        "        \"title\": \"Prompt Truncation\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"},\n",
        "    {\n",
        "        \"title\": \"Preambles\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/preambles\"}\n",
        "]"
      ],
      "metadata": {
        "id": "nv7_r3bJaWwv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt' data package using the NLTK Downloader\n",
        "# Specifying download_dir ensures it's saved to the desired location.\n",
        "nltk.download('punkt', download_dir='/root/nltk_data/tokenizers/punkt/PY3_tab')\n",
        "\n",
        "# Update the nltk data path\n",
        "nltk.data.path.append('/root/nltk_data/tokenizers/punkt/PY3_tab')\n",
        "\n",
        "class Vectorstore:\n",
        "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
        "        self.raw_documents = raw_documents\n",
        "        self.docs = []\n",
        "        self.docs_embs = []\n",
        "        self.retrieve_top_k = 10\n",
        "        self.rerank_top_k = 3\n",
        "        # Call load_and_chunk, embed, and index within __init__\n",
        "        self.load_and_chunk()\n",
        "        self.embed()\n",
        "        self.index()\n",
        "\n",
        "    def load_and_chunk(self) -> None:\n",
        "        \"\"\"\n",
        "        Loads the text from the sources and chunks the HTML content.\n",
        "        \"\"\"\n",
        "        print(\"Loading documents...\")\n",
        "\n",
        "        for raw_document in self.raw_documents:\n",
        "            elements = partition_html(url=raw_document[\"url\"])\n",
        "            chunks = chunk_by_title(elements)\n",
        "            for chunk in chunks:\n",
        "                self.docs.append(\n",
        "                    {\n",
        "                        \"title\": raw_document[\"title\"],\n",
        "                        \"text\": str(chunk),\n",
        "                        \"url\": raw_document[\"url\"],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    def embed(self) -> None:\n",
        "        \"\"\"\n",
        "        Embeds the document chunks using the Cohere API.\n",
        "        \"\"\"\n",
        "        print(\"Embedding document chunks...\")\n",
        "\n",
        "        batch_size = 90\n",
        "        self.docs_len = len(self.docs)\n",
        "        for i in range(0, self.docs_len, batch_size):\n",
        "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
        "            texts = [item[\"text\"] for item in batch]\n",
        "            docs_embs_batch = co.embed(\n",
        "                texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n",
        "            ).embeddings\n",
        "            self.docs_embs.extend(docs_embs_batch)\n",
        "\n",
        "\n",
        "    def index(self) -> None:\n",
        "        \"\"\"\n",
        "        Indexes the documents for efficient retrieval.\n",
        "        \"\"\"\n",
        "        print(\"Indexing documents...\")\n",
        "\n",
        "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
        "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
        "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
        "\n",
        "        print(f\"Indexing complete with {self.idx.get_current_count()} documents.\")\n",
        "\n",
        "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Retrieves document chunks based on the given query.\n",
        "\n",
        "        Parameters:\n",
        "        query (str): The query to retrieve document chunks for.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dense retrieval\n",
        "        query_emb = co.embed(\n",
        "            texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n",
        "        ).embeddings\n",
        "\n",
        "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
        "\n",
        "        # Reranking\n",
        "        rank_fields = [\"title\", \"text\"] # We'll use the title and text fields for reranking\n",
        "\n",
        "        docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids]\n",
        "\n",
        "        rerank_results = co.rerank(\n",
        "            query=query,\n",
        "            documents=docs_to_rerank,\n",
        "            top_n=self.rerank_top_k,\n",
        "            model=\"rerank-english-v3.0\",\n",
        "            rank_fields=rank_fields\n",
        "        )\n",
        "\n",
        "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvtiCnnJZrPW",
        "outputId": "26114561-11ae-491e-8b64-9a06b39d8e44"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /root/nltk_data/tokenizers/punkt/PY3_tab...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Vectorstore(raw_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvjRTHbBc5n5",
        "outputId": "1a1f242f-4727-4b72-f27d-aede5f0c98e7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents...\n",
            "Embedding document chunks...\n",
            "Indexing documents...\n",
            "Indexing complete with 138 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.retrieve(\"Prompting by giving examples\")"
      ],
      "metadata": {
        "id": "pZRKSYpscqnY"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatbot(message, chat_history=[]):\n",
        "\n",
        "    # Generate search queries, if any\n",
        "    response = co.chat(message=message,\n",
        "                        model=\"command-r-plus\",\n",
        "                        search_queries_only=True,\n",
        "                        chat_history=chat_history)\n",
        "\n",
        "    search_queries = []\n",
        "    for query in response.search_queries:\n",
        "        search_queries.append(query.text)\n",
        "\n",
        "    # If there are search queries, retrieve the documents\n",
        "    if search_queries:\n",
        "        print(\"Retrieving information...\", end=\"\")\n",
        "\n",
        "        # Retrieve document chunks for each query\n",
        "        documents = []\n",
        "        for query in search_queries:\n",
        "            # Check if the result of retrieve is not None before extending\n",
        "            retrieved_docs = vectorstore.retrieve(query)\n",
        "            if retrieved_docs is not None:\n",
        "                documents.extend(retrieved_docs)\n",
        "            else:\n",
        "                print(f\"Warning: No documents found for query: {query}\")  # Optional: Print a warning\n",
        "\n",
        "        # Use document chunks to respond\n",
        "        response = co.chat_stream(\n",
        "            message=message,\n",
        "            model=\"command-r-plus\",\n",
        "            documents=documents,\n",
        "            chat_history=chat_history,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        response = co.chat_stream(\n",
        "            message=message,\n",
        "            model=\"command-r-plus\",\n",
        "            chat_history=chat_history,\n",
        "        )\n",
        "\n",
        "    # Print the chatbot response and citations\n",
        "    chatbot_response = \"\"\n",
        "    print(\"\\nChatbot:\")\n",
        "\n",
        "    for event in response:\n",
        "        if event.event_type == \"text-generation\":\n",
        "            print(event.text, end=\"\")\n",
        "            chatbot_response += event.text\n",
        "        if event.event_type == \"stream-end\":\n",
        "            if event.response.citations:\n",
        "                print(\"\\n\\nCITATIONS:\")\n",
        "                for citation in event.response.citations:\n",
        "                    print(citation)\n",
        "            if event.response.documents:\n",
        "                print(\"\\nCITED DOCUMENTS:\")\n",
        "                for document in event.response.documents:\n",
        "                    print(document)\n",
        "            # Update the chat history for the next turn\n",
        "            chat_history = event.response.chat_history\n",
        "\n",
        "    return chat_history"
      ],
      "metadata": {
        "id": "58rTYIGZazfL"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = 382\n",
        "end = 397\n",
        "text = 'similar vectors'\n",
        "document_ids = ['doc_0', 'doc_2']"
      ],
      "metadata": {
        "id": "eKpESz5Cazcb"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn # 1\n",
        "chat_history = run_chatbot(\"Hello, I have a question\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5TsEH3WfC7Q",
        "outputId": "e83a443b-c80b-4e1c-fd9d-238330e2cfbf"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chatbot:\n",
            "Of course! I am here to help. Please go ahead with your question, and I will do my best to assist you."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn # 2\n",
        "chat_history = run_chatbot(\"What's the difference between zero-shot and few-shot prompting\", chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Utu-m7kGfOeZ",
        "outputId": "1e80e2e3-5be9-4aff-9eb0-116c842407dd"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving information...Warning: No documents found for query: zero-shot prompting\n",
            "Warning: No documents found for query: few-shot prompting\n",
            "\n",
            "Chatbot:\n",
            "Zero-shot and few-shot are both types of learning techniques used in machine learning and natural language processing. Here's the difference between the two:\n",
            "\n",
            "**Zero-Shot Learning:**\n",
            "Zero-shot learning refers to the ability of a model to perform a task without any explicit training examples from that task. In other words, the model has \"zero shots\" at learning from specific examples relevant to the task. Instead of relying on example input-output pairs, zero-shot learning relies on the model's ability to generalize and extrapolate from its pre-existing knowledge.\n",
            "\n",
            "For example, if you have a language model trained on a large corpus of text, you might ask it to classify sentiment (\"positive\" or \"negative\") for a given sentence, even though it hasn't been trained specifically on sentiment classification examples. The model is expected to understand and perform the task based on its inherent knowledge and understanding of language.\n",
            "\n",
            "**Few-Shot Learning:**\n",
            "Few-shot learning is a technique where a model is provided with a small number of training examples (hence \"few-shot\") for a specific task, and it then generalizes to perform that task for new, unseen examples. The model is given a set of input-output pairs as prompts, and it learns to map similar inputs to the correct outputs.\n",
            "\n",
            "Using the previous example, in few-shot learning, you would provide the language model with a few examples of sentences labeled with their corresponding sentiment (\"positive\" or \"negative\"). The model then learns from these examples and can classify the sentiment of new, unseen sentences.\n",
            "\n",
            "Few-shot learning is particularly useful when you don't have a large amount of labeled training data for a specific task. It allows the model to quickly adapt and learn new tasks with minimal supervision.\n",
            "\n",
            "In summary, the key difference is that zero-shot learning relies solely on the model's inherent knowledge without any task-specific examples, while few-shot learning provides a small number of examples to guide the model's performance on a new task."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn # 3\n",
        "chat_history = run_chatbot(\"How would the latter help?\", chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOX_-PCcfOcB",
        "outputId": "3fa07163-1ad9-42d1-e7e2-5e54f9c7b205"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving information...Warning: No documents found for query: benefits of few-shot learning\n",
            "\n",
            "Chatbot:\n",
            "Few-shot learning can be incredibly helpful in several ways:\n",
            "1. **Data Efficiency:** One of the primary advantages of few-shot learning is its ability to learn from a small number of examples. In many real-world scenarios, labeled training data is expensive and time-consuming to acquire. Few-shot learning allows models to learn and generalize from just a handful of examples, making it more feasible to apply machine learning to a broader range of tasks.\n",
            "2. **Adaptability:** Few-shot learning enables models to adapt quickly to new tasks. With just a few examples, the model can learn to perform a new classification or generation task. This adaptability is especially useful in dynamic environments where new categories or tasks emerge frequently.\n",
            "3. **Reduced Training Time:** Training large machine learning models from scratch can take a significant amount of time and computational resources. Few-shot learning reduces the training time by leveraging pre-trained models and fine-tuning them with a small number of examples. This makes it more practical to iterate and experiment with different approaches.\n",
            "4. **Human-like Learning:** Humans have the remarkable ability to learn new concepts from just a few examples. Few-shot learning attempts to mimic this aspect of human cognition, making models more human-like in their learning capabilities.\n",
            "5. **Improved Generalization:** Few-shot learning encourages models to generalize beyond the specific examples they were trained on. This generalization capability can lead to more robust models that can handle variations and nuances in the data.\n",
            "6. **Personalization:** Few-shot learning can be used to personalize models for individual users or specific use cases. For example, a language model can be fine-tuned with a few examples of a user's writing style to generate text that aligns with their preferences.\n",
            "7. **Handling Long-tail Phenomena:** In many applications, there are numerous classes or categories that have very few examples. Few-shot learning can be particularly useful in handling these long-tail phenomena, where traditional supervised learning methods may struggle due to the lack of sufficient training data.\n",
            "\n",
            "Few-shot learning is an active area of research in machine learning and natural language processing, and advancements in this field have the potential to make models more flexible, adaptable, and efficient."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn # 4\n",
        "chat_history = run_chatbot(\"What do you know about 5G networks?\", chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEH8kkEufktq",
        "outputId": "9647d583-9bc6-4d15-bf6c-832bd4131081"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving information...Warning: No documents found for query: 5G networks explained\n",
            "\n",
            "Chatbot:\n",
            "5G networks refer to the fifth generation of mobile communication technology, designed as the successor to 4G (LTE) networks. 5G brings significant improvements in terms of speed, latency, capacity, and connectivity compared to its predecessors. Here are some key aspects of 5G networks:\n",
            "\n",
            "1. **Speed:** 5G offers much faster data transfer rates compared to 4G, with peak speeds reaching up to 10 gigabits per second (Gbps) or more. In practical terms, this means faster download and upload times, seamless streaming of high-resolution videos, and quicker file transfers.\n",
            "2. **Low Latency:** Latency refers to the delay between sending and receiving data. 5G networks are designed to have extremely low latency, which can be as low as 1 millisecond or even less. This makes 5G ideal for applications that require real-time responsiveness, such as self-driving cars, industrial automation, and virtual reality.\n",
            "3. **Increased Capacity:** 5G networks can support a significantly higher number of connected devices per square kilometer compared to 4G. This increased capacity enables the Internet of Things (IoT) by allowing a vast number of devices, sensors, and appliances to connect and communicate with each other.\n",
            "4. **Network Slicing:** 5G introduces the concept of network slicing, which allows the creation of multiple virtual networks on top of a common physical infrastructure. Each virtual network, or \"slice,\" can be customized with specific characteristics, such as bandwidth, latency, and security, to meet the diverse needs of different applications and industries.\n",
            "5. **Millimeter-Wave Spectrum:** 5G utilizes higher frequency bands, including the millimeter-wave spectrum, to achieve its high speeds and capacity. These higher frequencies have shorter ranges, which is why 5G networks require a denser deployment of base stations and antennas.\n",
            "6. **Massive MIMO (Multiple-Input Multiple-Output):** 5G networks employ Massive MIMO technology, where base stations use a large number of antennas to transmit and receive multiple data streams simultaneously. This improves spectral efficiency and increases network capacity.\n",
            "7. **Beamforming:** Beamforming is a technique where a base station sends focused, directional signals directly to specific devices, rather than broadcasting signals in all directions. This improves signal strength and reduces interference.\n",
            "8. **Edge Computing:** 5G networks are often coupled with edge computing, which brings data processing and storage closer to the end-user or device. This reduces latency even further and enables applications that require real-time data processing, such as augmented reality.\n",
            "9. **Enhanced Security:** 5G networks incorporate improved security features, including better authentication, encryption, and privacy protection. These enhancements are crucial as 5G enables a wider range of applications, including those that handle sensitive data.\n",
            "10. **Use Cases:** 5G technology is expected to revolutionize a variety of industries, including telecommunications, healthcare, transportation, manufacturing, entertainment, and smart cities. It will enable new applications and services that were not feasible with previous generations of mobile networks.\n",
            "\n",
            "It's important to note that the rollout of 5G networks is still ongoing, and the availability and performance of 5G services can vary depending on geographical location and network provider."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chat history:\")\n",
        "for c in chat_history:\n",
        "    print(c, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uImCZaspfnJY",
        "outputId": "cdb4c2e7-7a98-40fb-e01b-2db72aae0d5b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat history:\n",
            "role='USER' message='Hello, I have a question' tool_calls=None \n",
            "\n",
            "role='CHATBOT' message='Of course! I am here to help. Please go ahead with your question, and I will do my best to assist you.' tool_calls=None \n",
            "\n",
            "role='USER' message=\"What's the difference between zero-shot and few-shot prompting\" tool_calls=None \n",
            "\n",
            "role='CHATBOT' message='Zero-shot and few-shot are both types of learning techniques used in machine learning and natural language processing. Here\\'s the difference between the two:\\n\\n**Zero-Shot Learning:**\\nZero-shot learning refers to the ability of a model to perform a task without any explicit training examples from that task. In other words, the model has \"zero shots\" at learning from specific examples relevant to the task. Instead of relying on example input-output pairs, zero-shot learning relies on the model\\'s ability to generalize and extrapolate from its pre-existing knowledge.\\n\\nFor example, if you have a language model trained on a large corpus of text, you might ask it to classify sentiment (\"positive\" or \"negative\") for a given sentence, even though it hasn\\'t been trained specifically on sentiment classification examples. The model is expected to understand and perform the task based on its inherent knowledge and understanding of language.\\n\\n**Few-Shot Learning:**\\nFew-shot learning is a technique where a model is provided with a small number of training examples (hence \"few-shot\") for a specific task, and it then generalizes to perform that task for new, unseen examples. The model is given a set of input-output pairs as prompts, and it learns to map similar inputs to the correct outputs.\\n\\nUsing the previous example, in few-shot learning, you would provide the language model with a few examples of sentences labeled with their corresponding sentiment (\"positive\" or \"negative\"). The model then learns from these examples and can classify the sentiment of new, unseen sentences.\\n\\nFew-shot learning is particularly useful when you don\\'t have a large amount of labeled training data for a specific task. It allows the model to quickly adapt and learn new tasks with minimal supervision.\\n\\nIn summary, the key difference is that zero-shot learning relies solely on the model\\'s inherent knowledge without any task-specific examples, while few-shot learning provides a small number of examples to guide the model\\'s performance on a new task.' tool_calls=None \n",
            "\n",
            "role='USER' message='How would the latter help?' tool_calls=None \n",
            "\n",
            "role='CHATBOT' message=\"Few-shot learning can be incredibly helpful in several ways:\\n1. **Data Efficiency:** One of the primary advantages of few-shot learning is its ability to learn from a small number of examples. In many real-world scenarios, labeled training data is expensive and time-consuming to acquire. Few-shot learning allows models to learn and generalize from just a handful of examples, making it more feasible to apply machine learning to a broader range of tasks.\\n2. **Adaptability:** Few-shot learning enables models to adapt quickly to new tasks. With just a few examples, the model can learn to perform a new classification or generation task. This adaptability is especially useful in dynamic environments where new categories or tasks emerge frequently.\\n3. **Reduced Training Time:** Training large machine learning models from scratch can take a significant amount of time and computational resources. Few-shot learning reduces the training time by leveraging pre-trained models and fine-tuning them with a small number of examples. This makes it more practical to iterate and experiment with different approaches.\\n4. **Human-like Learning:** Humans have the remarkable ability to learn new concepts from just a few examples. Few-shot learning attempts to mimic this aspect of human cognition, making models more human-like in their learning capabilities.\\n5. **Improved Generalization:** Few-shot learning encourages models to generalize beyond the specific examples they were trained on. This generalization capability can lead to more robust models that can handle variations and nuances in the data.\\n6. **Personalization:** Few-shot learning can be used to personalize models for individual users or specific use cases. For example, a language model can be fine-tuned with a few examples of a user's writing style to generate text that aligns with their preferences.\\n7. **Handling Long-tail Phenomena:** In many applications, there are numerous classes or categories that have very few examples. Few-shot learning can be particularly useful in handling these long-tail phenomena, where traditional supervised learning methods may struggle due to the lack of sufficient training data.\\n\\nFew-shot learning is an active area of research in machine learning and natural language processing, and advancements in this field have the potential to make models more flexible, adaptable, and efficient.\" tool_calls=None \n",
            "\n",
            "role='USER' message='What do you know about 5G networks?' tool_calls=None \n",
            "\n",
            "role='CHATBOT' message='5G networks refer to the fifth generation of mobile communication technology, designed as the successor to 4G (LTE) networks. 5G brings significant improvements in terms of speed, latency, capacity, and connectivity compared to its predecessors. Here are some key aspects of 5G networks:\\n\\n1. **Speed:** 5G offers much faster data transfer rates compared to 4G, with peak speeds reaching up to 10 gigabits per second (Gbps) or more. In practical terms, this means faster download and upload times, seamless streaming of high-resolution videos, and quicker file transfers.\\n2. **Low Latency:** Latency refers to the delay between sending and receiving data. 5G networks are designed to have extremely low latency, which can be as low as 1 millisecond or even less. This makes 5G ideal for applications that require real-time responsiveness, such as self-driving cars, industrial automation, and virtual reality.\\n3. **Increased Capacity:** 5G networks can support a significantly higher number of connected devices per square kilometer compared to 4G. This increased capacity enables the Internet of Things (IoT) by allowing a vast number of devices, sensors, and appliances to connect and communicate with each other.\\n4. **Network Slicing:** 5G introduces the concept of network slicing, which allows the creation of multiple virtual networks on top of a common physical infrastructure. Each virtual network, or \"slice,\" can be customized with specific characteristics, such as bandwidth, latency, and security, to meet the diverse needs of different applications and industries.\\n5. **Millimeter-Wave Spectrum:** 5G utilizes higher frequency bands, including the millimeter-wave spectrum, to achieve its high speeds and capacity. These higher frequencies have shorter ranges, which is why 5G networks require a denser deployment of base stations and antennas.\\n6. **Massive MIMO (Multiple-Input Multiple-Output):** 5G networks employ Massive MIMO technology, where base stations use a large number of antennas to transmit and receive multiple data streams simultaneously. This improves spectral efficiency and increases network capacity.\\n7. **Beamforming:** Beamforming is a technique where a base station sends focused, directional signals directly to specific devices, rather than broadcasting signals in all directions. This improves signal strength and reduces interference.\\n8. **Edge Computing:** 5G networks are often coupled with edge computing, which brings data processing and storage closer to the end-user or device. This reduces latency even further and enables applications that require real-time data processing, such as augmented reality.\\n9. **Enhanced Security:** 5G networks incorporate improved security features, including better authentication, encryption, and privacy protection. These enhancements are crucial as 5G enables a wider range of applications, including those that handle sensitive data.\\n10. **Use Cases:** 5G technology is expected to revolutionize a variety of industries, including telecommunications, healthcare, transportation, manufacturing, entertainment, and smart cities. It will enable new applications and services that were not feasible with previous generations of mobile networks.\\n\\nIt\\'s important to note that the rollout of 5G networks is still ongoing, and the availability and performance of 5G services can vary depending on geographical location and network provider.' tool_calls=None \n",
            "\n"
          ]
        }
      ]
    }
  ]
}